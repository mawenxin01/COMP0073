#!/usr/bin/env python3
"""
Step 5: Apply Llama classification results to full dataset
Map Llama-3.3-70B classification results of unique chief complaints to original records
"""

import pandas as pd
import numpy as np
import json
import os
import logging
from datetime import datetime

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ClassificationMapper:
    """Classification result mapper"""
    
    def __init__(self):
        """Initialize mapper"""
        # Get script directory, then build data directory path
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        
        self.data_dir = os.path.join(project_root, "data")
        self.output_dir = os.path.join(self.data_dir, "final_results")
        
        # Create output directory
        os.makedirs(self.output_dir, exist_ok=True)
        
        logger.info("âœ… Classification result mapper initialized")
    
    def load_original_data(self):
        """Load original 400k data"""
        logger.info("ğŸ“‚ Loading original data...")
        
        try:
            original_file = os.path.join(self.data_dir, "processed_data", "triage_final.csv")
            df = pd.read_csv(original_file)
            
            logger.info(f"âœ… Original data loaded: {len(df):,} records")
            logger.info(f"   Unique chief complaints: {df['chiefcomplaint_clean'].nunique():,}")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ Failed to load original data: {e}")
            raise
    
    def load_classification_results(self):
        """Load classification results"""
        logger.info("ğŸ“‚ Loading classification results...")
        
        try:
            # Find latest classification results
            two_stage_dir = os.path.join(self.data_dir, "two_stage_classification")
            latest_file = os.path.join(two_stage_dir, "latest_two_stage.json")
            
            if os.path.exists(latest_file):
                with open(latest_file, 'r') as f:
                    metadata = json.load(f)
                
                results_file = metadata['results_file']
                results_df = pd.read_csv(results_file)
                
                logger.info(f"âœ… Classification results loaded: {len(results_df):,} unique chief complaints")
                logger.info(f"   Results file: {results_file}")
                
                return results_df, metadata
            else:
                raise FileNotFoundError("Classification results file not found")
                
        except Exception as e:
            logger.error(f"âŒ Failed to load classification results: {e}")
            raise
    
    def create_classification_lookup(self, classification_df):
        """Create classification lookup dictionary"""
        logger.info("ğŸ” Creating classification lookup table...")
        
        # Create mapping dictionary from chief complaint to classification results
        lookup_dict = {}
        
        for _, row in classification_df.iterrows():
            complaint = row['chiefcomplaint_clean']
            lookup_dict[complaint] = {
                'group_id': row['group_id'],
                'group_name': row['group_name'],
                'group_confidence': row['group_confidence'],
                'severity': row['severity'],
                'severity_confidence': row['severity_confidence']
            }
        
        logger.info(f"âœ… Lookup table created: {len(lookup_dict):,} mapping relationships")
        
        return lookup_dict
    
    def apply_classification_to_full_data(self, original_df, lookup_dict):
        """Apply classification results to full dataset"""
        logger.info("ğŸ¯ Starting to map classification results to full dataset...")
        
        # Copy original data
        result_df = original_df.copy()
        
        # Add classification result columns
        result_df['group_id'] = None
        result_df['group_name'] = None
        result_df['group_confidence'] = None
        result_df['severity'] = None
        result_df['severity_confidence'] = None
        result_df['classification_found'] = False
        
        # Statistics variables
        found_count = 0
        missing_count = 0
        missing_complaints = set()
        
        # Map row by row
        logger.info("   Performing data mapping...")
        for idx, row in result_df.iterrows():
            complaint = row['chiefcomplaint_clean']
            
            if complaint in lookup_dict:
                # Found matching classification results
                classification = lookup_dict[complaint]
                result_df.at[idx, 'group_id'] = classification['group_id']
                result_df.at[idx, 'group_name'] = classification['group_name']
                result_df.at[idx, 'group_confidence'] = classification['group_confidence']
                result_df.at[idx, 'severity'] = classification['severity']
                result_df.at[idx, 'severity_confidence'] = classification['severity_confidence']
                result_df.at[idx, 'classification_found'] = True
                found_count += 1
            else:
                # No matching classification results found
                missing_count += 1
                missing_complaints.add(complaint)
            
            # Progress update
            if (idx + 1) % 50000 == 0:
                logger.info(f"   Processed: {idx + 1:,}/{len(result_df):,} ({(idx + 1)/len(result_df)*100:.1f}%)")
        
        # Mapping completion statistics
        logger.info("âœ… Data mapping completed!")
        logger.info(f"   Successfully mapped: {found_count:,} records ({found_count/len(result_df)*100:.2f}%)")
        logger.info(f"   Classification not found: {missing_count:,} records ({missing_count/len(result_df)*100:.2f}%)")
        logger.info(f"   Missing unique chief complaints: {len(missing_complaints):,}")

        
        return result_df, missing_complaints
    
    def generate_mapping_statistics(self, result_df, missing_complaints, metadata):
        """Generate mapping statistics report"""
        logger.info("ğŸ“Š Generating statistics report...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Basic statistics
        total_records = len(result_df)
        mapped_records = result_df['classification_found'].sum()
        unmapped_records = total_records - mapped_records
        
        # Symptom group distribution
        group_stats = result_df[result_df['classification_found']].groupby(['group_id', 'group_name']).size().sort_values(ascending=False)
        
        # Severity distribution (with robust handling for mixed types)
        severity_series = result_df[result_df['classification_found']]['severity']
        
        # Convert to numeric, coercing non-numeric values to NaN
        severity_numeric = pd.to_numeric(severity_series, errors='coerce')
        
        # Drop NaN values and calculate statistics
        severity_stats = severity_numeric.dropna().value_counts().sort_index()
        
        # Generate report
        report_file = os.path.join(self.output_dir, f"mapping_report_{timestamp}.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Classification Results Mapping Report\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Generated time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Classification results used: {metadata.get('results_file', 'Unknown')}\n")
            f.write(f"Classification model: {metadata.get('model_used', 'Unknown')}\n\n")
            
            f.write("Mapping statistics:\n")
            f.write("-" * 30 + "\n")
            f.write(f"Total original records: {total_records:,}\n")
            f.write(f"Successfully mapped records: {mapped_records:,} ({mapped_records/total_records*100:.2f}%)\n")
            f.write(f"Unmapped records: {unmapped_records:,} ({unmapped_records/total_records*100:.2f}%)\n")
            f.write(f"Missing unique chief complaints: {len(missing_complaints):,}\n\n")
            
            f.write("Symptom group distribution (mapped records):\n")
            f.write("-" * 30 + "\n")
            for (group_id, group_name), count in group_stats.items():
                percentage = count / mapped_records * 100
                f.write(f"Group {group_id} ({group_name}): {count:,} ({percentage:.1f}%)\n")
            
            f.write(f"\nSeverity distribution (mapped records):\n")
            f.write("-" * 30 + "\n")
            severity_names = {1: "Critical", 2: "Severe", 3: "Moderate", 4: "Mild", 5: "Minor"}
            for severity, count in severity_stats.items():
                name = severity_names.get(severity, f"Level {severity}")
                percentage = count / mapped_records * 100
                f.write(f"{severity} level ({name}): {count:,} ({percentage:.1f}%)\n")
            
            if missing_complaints:
                f.write(f"\nMissing chief complaint examples (first 20):\n")
                f.write("-" * 30 + "\n")
                for i, complaint in enumerate(list(missing_complaints)[:20], 1):
                    f.write(f"{i}. {complaint}\n")
        
        logger.info(f"ğŸ“‹ Statistics report saved: {report_file}")
        return report_file
    
    def save_final_results(self, result_df, missing_complaints, metadata):
        """Save final results"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save complete results
        full_results_file = os.path.join(self.output_dir, f"triage_with_classification_{timestamp}.csv")
        result_df.to_csv(full_results_file, index=False)
        logger.info(f"ğŸ’¾ Complete results saved: {full_results_file}")
        
        # Save only successfully mapped records
        mapped_results_file = os.path.join(self.output_dir, f"triage_mapped_only_{timestamp}.csv")
        mapped_df = result_df[result_df['classification_found']].copy()
        mapped_df = mapped_df.drop('classification_found', axis=1)  # Remove helper column
        mapped_df.to_csv(mapped_results_file, index=False)
        logger.info(f"ğŸ’¾ Mapped records saved: {mapped_results_file}")
        
        # Save missing chief complaints list
        if missing_complaints:
            missing_file = os.path.join(self.output_dir, f"missing_complaints_{timestamp}.txt")
            with open(missing_file, 'w', encoding='utf-8') as f:
                f.write("Chief complaints without classification results\n")
                f.write("=" * 30 + "\n")
                # Convert all items to string before sorting to avoid TypeError with mixed types (e.g., str and float NaN)
                sorted_complaints = sorted([str(c) for c in missing_complaints])
                for complaint in sorted_complaints:
                    f.write(f"{complaint}\n")
            logger.info(f"ğŸ“‹ Missing chief complaints list saved: {missing_file}")
        
        # Generate statistics report
        report_file = self.generate_mapping_statistics(result_df, missing_complaints, metadata)
        
        # Save metadata
        final_metadata = {
            'timestamp': timestamp,
            'source_classification': metadata.get('results_file'),
            'original_records': len(result_df),
            # Cast numpy.int64 to standard python int for JSON serialization
            'mapped_records': int(result_df['classification_found'].sum()),
            'unmapped_records': int(len(result_df) - result_df['classification_found'].sum()),
            'missing_complaints_count': len(missing_complaints),
            'full_results_file': full_results_file,
            'mapped_only_file': mapped_results_file,
            'report_file': report_file,
            'model_used': metadata.get('model_used'),
            'method': metadata.get('method')
        }
        
        metadata_file = os.path.join(self.output_dir, f"mapping_metadata_{timestamp}.json")
        with open(metadata_file, 'w') as f:
            json.dump(final_metadata, f, indent=2)
        
        # Update latest pointer
        latest_file = os.path.join(self.output_dir, "latest_mapping.json")
        with open(latest_file, 'w') as f:
            json.dump(final_metadata, f, indent=2)
        
        logger.info(f"ğŸ“‹ Metadata saved: {metadata_file}")
        
        return final_metadata
    
    def process_full_mapping(self):
        """Complete mapping processing flow"""
        logger.info("ğŸ¯ Starting to map classification results to full dataset")
        logger.info("=" * 60)
        
        # Step 1: Load original data
        original_df = self.load_original_data()
        
        # Step 2: Load classification results
        classification_df, metadata = self.load_classification_results()
        
        # Step 3: Create lookup table
        lookup_dict = self.create_classification_lookup(classification_df)
        
        # Step 4: Apply classification to full dataset
        result_df, missing_complaints = self.apply_classification_to_full_data(original_df, lookup_dict)
        
        # Step 5: Save results
        final_metadata = self.save_final_results(result_df, missing_complaints, metadata)
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ Classification result mapping completed!")
        logger.info(f"ğŸ“Š Mapping statistics:")
        logger.info(f"   Original records: {final_metadata['original_records']:,}")
        logger.info(f"   Successfully mapped: {final_metadata['mapped_records']:,} ({final_metadata['mapped_records']/final_metadata['original_records']*100:.2f}%)")
        logger.info(f"   Unmapped records: {final_metadata['unmapped_records']:,}")
        logger.info(f"ğŸ“‚ Main output files:")
        logger.info(f"   Complete results: {final_metadata['full_results_file']}")
        logger.info(f"   Mapped records: {final_metadata['mapped_only_file']}")
        logger.info(f"   Statistics report: {final_metadata['report_file']}")
        
        return final_metadata


def main():
    """Main function"""
    print("ğŸ¥ Classification Results Mapping Tool")
    print("ğŸ“Š Apply unique chief complaint classification results to full 400k dataset")
    print("=" * 60)
    
    try:
        # Create mapper
        mapper = ClassificationMapper()
        
        # Execute mapping processing
        result = mapper.process_full_mapping()
        
        print("\n" + "=" * 60)
        print("âœ… Mapping processing completed!")
        print(f"ğŸ“Š Successfully processed {result['original_records']:,} records")
        print(f"ğŸ“ˆ Mapping success rate: {result['mapped_records']/result['original_records']*100:.2f}%")
        print(f"ğŸ“‚ Detailed report: {result['report_file']}")
        print("\nğŸ’¡ Now you can:")
        print("   1. View mapping statistics report")
        print("   2. Analyze symptom group and severity distribution")
        print("   3. Handle unmapped records")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ Error occurred during processing: {e}")
        print("ğŸ’¡ Please ensure:")
        print("   1. Unique chief complaint classification has been completed")
        print("   2. Original data file exists")
        print("   3. Sufficient disk space available")


if __name__ == "__main__":
    main() 

# ğŸ“ Step 5: æ˜ å°„åˆ†ç±»ç»“æœåˆ°å®Œæ•´æ•°æ®é›†ï¼ˆClassificationMapperï¼‰

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚               process_full_mapping()         â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                      â†“
# 1ï¸âƒ£ Load åŸå§‹ä¸»è¯‰æ•°æ®ï¼ˆ400kï¼‰
#    â””â”€â”€ original_df = load_original_data()

#                      â†“
# 2ï¸âƒ£ Load ä¸¤é˜¶æ®µåˆ†ç±»ç»“æœï¼ˆ55k å”¯ä¸€ä¸»è¯‰ï¼‰
#    â””â”€â”€ classification_df, metadata = load_classification_results()

#                      â†“
# 3ï¸âƒ£ æ„å»ºæ˜ å°„è¡¨ lookup_dict
#    â””â”€â”€ chiefcomplaint_clean â†’ group_id, severity ç­‰
#    â””â”€â”€ lookup_dict = create_classification_lookup(classification_df)

#                      â†“
# 4ï¸âƒ£ å¯¹åŸå§‹æ•°æ®é€æ¡æ˜ å°„ï¼ˆapply_classification_to_full_dataï¼‰
#    â”Œâ”€ éå†æ¯ä¸€è¡Œ chiefcomplaint_clean:
#    â”‚    â”œâ”€ å¦‚æœèƒ½åœ¨ lookup_dict ä¸­æ‰¾åˆ° â†’ èµ‹å€¼ group_idã€severity ç­‰
#    â”‚    â””â”€ å¦åˆ™è®¡å…¥ missing_complaints
#    â””â”€ è¾“å‡º result_df å’Œ missing chief complaints

#                      â†“
# 5ï¸âƒ£ ä¿å­˜ç»“æœï¼ˆsave_final_resultsï¼‰
#    â”œâ”€ ä¿å­˜å®Œæ•´æ•°æ®ï¼ˆtriage_with_classification_xxx.csvï¼‰
#    â”œâ”€ ä¿å­˜å·²æ˜ å°„å­é›†ï¼ˆtriage_mapped_only_xxx.csvï¼‰
#    â”œâ”€ ä¿å­˜ç¼ºå¤±ä¸»è¯‰åˆ—è¡¨ï¼ˆmissing_complaints_xxx.txtï¼‰
#    â””â”€ ä¿å­˜ç»Ÿè®¡æŠ¥å‘Šï¼ˆmapping_report_xxx.txtï¼‰

#                      â†“
# 6ï¸âƒ£ è¾“å‡ºæœ€ç»ˆ metadata & æ˜¾ç¤ºæ˜ å°„æ¦‚å†µ

# âœ… æœ€ç»ˆç»“æœï¼š
#    - å®Œæ•´æ˜ å°„æ•°æ®æ–‡ä»¶ï¼ˆå« group_id + severityï¼‰
#    - å¯åˆ†æç—‡çŠ¶åˆ†å¸ƒã€ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
#    - å¯è¿½è¸ªç¼ºå¤±ä¸»è¯‰åŸå› 
 